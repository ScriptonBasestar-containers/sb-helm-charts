# Default values for minio.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# ==================================================================================
# DEPLOYMENT SCENARIOS
# ==================================================================================
# This file contains default values. For production-ready configurations, see:
# - values-home-single.yaml: Home server/personal use (1 node, 2 drives)
# - values-startup-single.yaml: Small teams (1 node, 4 drives)
# - values-prod-distributed.yaml: Production HA (4+ nodes, distributed erasure coding)
# See: charts/minio/README.md for complete documentation
# ==================================================================================

# Deployment mode
# standalone: Single-server deployment (1 node, development/testing)
# distributed: Multi-server distributed deployment (4+ nodes, production/HA with erasure coding)
# Note: Distributed mode requires replicaCount >= 4 and even number of total drives
replicaCount: 1

# Pod management policy for StatefulSet
# Parallel: Pods are created/deleted in parallel (faster but less ordered)
# OrderedReady: Pods are created/deleted in order (slower but more controlled)
podManagementPolicy: Parallel

# Update strategy
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    partition: 0

# Cluster domain for DNS resolution
clusterDomain: cluster.local

# MinIO configuration
minio:
  # Deployment mode: standalone or distributed
  # - standalone: Single server with one or more drives (development)
  # - distributed: 4+ servers with erasure coding (production)
  mode: standalone

  # Number of drives per node
  # Standalone: Typically 1-2 drives
  # Distributed: Recommended 4+ drives for better erasure coding
  drivesPerNode: 1

  # Root credentials
  rootUser: "admin"
  rootPassword: "minio123"  # REQUIRED - Set this or use existingSecret

  # Use an existing secret for credentials
  existingSecret: ""
  rootUserKey: "root-user"
  rootPasswordKey: "root-password"

  # Secret annotations (for tools like external-secrets)
  secretAnnotations: {}

  # Use an existing ConfigMap
  existingConfigMap: ""

  # ConfigMap annotations
  configMapAnnotations: {}

  # Additional environment configuration
  # This will be mounted as config.env if provided
  config: ""

  # Additional config data
  configData: {}

  # MinIO region
  region: "us-east-1"

  # Browser redirect URL (for console behind proxy/ingress)
  browserRedirectURL: ""

  # Prometheus configuration
  prometheusAuthType: ""  # "public" to allow unauthenticated access
  prometheusURL: ""
  prometheusJobID: "minio-job"

  # Additional MinIO server arguments
  extraArgs: []
    # - --ftp="address=:8021"
    # - --sftp="address=:8022"

# Image configuration
image:
  repository: minio/minio
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# ServiceAccount configuration
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# RBAC configuration
rbac:
  # Specifies whether RBAC resources should be created
  create: true
  # Annotations to add to the Role and RoleBinding
  annotations: {}

# Pod annotations
podAnnotations: {}

# Pod labels
podLabels: {}

# Pod security context
# MinIO runs as non-root user (UID 1000) for security
podSecurityContext:
  fsGroup: 1000         # Group ownership for volume mounts
  runAsUser: 1000       # MinIO default user
  runAsGroup: 1000      # MinIO default group
  fsGroupChangePolicy: "OnRootMismatch"  # Only change permissions if mismatch (faster)

# Container security context
# Enforces security best practices (non-root, no privileged capabilities)
securityContext:
  runAsNonRoot: true    # Prevent running as root
  runAsUser: 1000       # MinIO user UID
  capabilities:
    drop:
      - ALL           # Drop all Linux capabilities for security
  # readOnlyRootFilesystem: true  # Enable for enhanced security (MinIO needs writable tmp)

# Service configuration
service:
  type: ClusterIP
  clusterIP: ""
  annotations: {}
  externalIPs: []
  loadBalancerIP: ""
  loadBalancerSourceRanges: []
  externalTrafficPolicy: ""
  sessionAffinity: ""
  sessionAffinityConfig: {}

  api:
    port: 9000
    nodePort: ""
  console:
    port: 9001
    nodePort: ""

# Headless service configuration (for distributed mode)
headlessService:
  annotations: {}

# Ingress configuration
ingress:
  # API ingress (S3 endpoint)
  api:
    enabled: false
    className: ""
    annotations: {}
      # cert-manager.io/cluster-issuer: letsencrypt-prod
      # nginx.ingress.kubernetes.io/proxy-body-size: "0"
    hosts:
      - host: minio-api.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: []
      # - secretName: minio-api-tls
      #   hosts:
      #     - minio-api.example.com

  # Console ingress (Web UI)
  console:
    enabled: false
    className: ""
    annotations: {}
      # cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - host: minio-console.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: []
      # - secretName: minio-console-tls
      #   hosts:
      #     - minio-console.example.com

# Resource limits and requests
resources:
  limits:
    cpu: 2000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 512Mi

# Liveness probe configuration
livenessProbe:
  httpGet:
    path: /minio/health/live
    port: api
    scheme: HTTP
  initialDelaySeconds: 30
  periodSeconds: 20
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3

# Readiness probe configuration
readinessProbe:
  httpGet:
    path: /minio/health/ready
    port: api
    scheme: HTTP
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3

# Startup probe configuration
startupProbe:
  httpGet:
    path: /minio/health/live
    port: api
    scheme: HTTP
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 30

# Autoscaling configuration
# Note: StatefulSet autoscaling requires Kubernetes 1.27+
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Persistence configuration
# Storage per drive (total = size × drivesPerNode × replicaCount)
# Example: 100Gi × 4 drives × 4 nodes = 1.6TB total in distributed mode
persistence:
  enabled: true
  storageClass: ""      # Use default storage class or specify (e.g., "fast-ssd")
  accessMode: ReadWriteOnce
  size: 100Gi           # Size per drive (recommended: 100Gi-500Gi for production)
  annotations: {}
  # Optionally use an existing PVC
  # Note: This only works for standalone mode with single drive
  existingClaim: ""

# PodDisruptionBudget configuration
podDisruptionBudget:
  enabled: false
  minAvailable: 1
  # maxUnavailable: 1
  unhealthyPodEvictionPolicy: ""

# NetworkPolicy configuration
networkPolicy:
  enabled: false
  ingress:
    api:
      from: []
        # - namespaceSelector:
        #     matchLabels:
        #       name: myapp
    console:
      from: []
    extraRules: []
  egress:
    extraRules: []

# Monitoring configuration
monitoring:
  enabled: false
  serviceMonitor:
    enabled: false
    namespace: ""
    labels: {}
    annotations: {}
    interval: 30s
    scrapeTimeout: 10s
    scheme: http
    tlsConfig: {}
    bearerTokenFile: ""
    bearerTokenSecret: {}
    relabelings: []
    metricRelabelings: []

# Node selector
nodeSelector: {}

# Tolerations
tolerations: []

# Affinity
affinity: {}

# Extra environment variables
extraEnv: []
  # - name: CUSTOM_VAR
  #   value: "value"

# Extra environment variables from ConfigMaps or Secrets
extraEnvFrom: []
  # - configMapRef:
  #     name: minio-extra-config
  # - secretRef:
  #     name: minio-extra-secret

# Extra volumes
extraVolumes: []
  # - name: extra-config
  #   configMap:
  #     name: extra-config

# Extra volume mounts
extraVolumeMounts: []
  # - name: extra-config
  #   mountPath: /extra-config
  #   readOnly: true

# Init containers
initContainers: []
  # - name: init-chmod
  #   image: busybox:latest
  #   command: ['sh', '-c', 'chown -R 1000:1000 /data']
  #   volumeMounts:
  #     - name: data-0
  #       mountPath: /data

# Lifecycle hooks
lifecycle: {}
  # preStop:
  #   exec:
  #     command: ["/bin/sh", "-c", "sleep 5"]

# ==================================================================================
# BACKUP & RECOVERY
# ==================================================================================
# This section is for documentation only. MinIO backups are performed manually
# via Makefile targets (make -f make/ops/minio.mk minio-*-backup).
# See: docs/minio-backup-guide.md for comprehensive backup/recovery procedures.
#
# Backup Components:
# - Bucket Data: Object storage, versions, metadata, multipart uploads (Critical)
# - Bucket Metadata: Policies, versioning, lifecycle rules, replication (Critical)
# - Configuration: Server config, environment variables, Kubernetes resources (Important)
# - IAM Policies: Users, groups, policies, access keys (Important)
#
# Backup Methods:
# - Site Replication: Continuous multi-site replication (production HA)
# - Bucket Replication: Selective bucket-level replication (S3-compatible)
# - mc mirror: Scheduled sync using MinIO client (simple, effective)
# - mc cp: Object-level copy (selective backups)
# - PVC Snapshots: Kubernetes VolumeSnapshots (instant, storage-level)
# - Restic: Incremental backups with deduplication (efficient, flexible)
#
# Quick Commands:
#   make -f make/ops/minio.mk minio-full-backup          # All components
#   make -f make/ops/minio.mk minio-backup-buckets       # Bucket data
#   make -f make/ops/minio.mk minio-backup-config        # Configuration
#   make -f make/ops/minio.mk minio-backup-iam           # IAM policies
#   make -f make/ops/minio.mk minio-backup-metadata      # Bucket metadata
#
# Recovery Time Objectives (RTO) / Recovery Point Objectives (RPO):
# - Bucket Metadata: RTO <30 min, RPO 24 hours
# - Configuration: RTO <15 min, RPO 24 hours
# - IAM Policies: RTO <15 min, RPO 24 hours
# - Full Disaster Recovery: RTO <2 hours, RPO 24 hours
# - Object-level Recovery: RTO <1 hour, RPO Real-time (with versioning)
# ==================================================================================
backup:
  # enabled: false - Documentation only, no automated CronJobs
  # All backups are performed manually via Makefile targets
  documentation:
    strategy: "Bucket Data + Bucket Metadata + Configuration + IAM Policies"
    tools:
      - "mc (MinIO Client)"
      - "kubectl (Kubernetes CLI)"
      - "VolumeSnapshots (CSI driver)"
      - "Restic (optional)"
    components:
      bucket_data: "Object storage, versions, metadata, multipart uploads (Critical)"
      bucket_metadata: "Policies, versioning, lifecycle rules, replication (Critical)"
      configuration: "Server config, environment variables, Kubernetes resources (Important)"
      iam_policies: "Users, groups, policies, access keys (Important)"
    targets:
      rto: "< 2 hours (full disaster recovery)"
      rpo: "24 hours (daily backups recommended)"

# ==================================================================================
# UPGRADE PROCEDURES
# ==================================================================================
# This section is for documentation only. MinIO upgrades are performed manually
# via Helm upgrade commands with pre/post validation via Makefile targets.
# See: docs/minio-upgrade-guide.md for comprehensive upgrade procedures.
#
# Upgrade Strategies:
# 1. Rolling Upgrade (Zero Downtime):
#    - For distributed mode (4+ nodes)
#    - Pods upgraded sequentially while maintaining quorum
#    - Recommended for production deployments
#    - Downtime: None
#
# 2. In-Place Upgrade (Planned Downtime):
#    - For standalone mode (1 node)
#    - Simple upgrade with brief service interruption
#    - Downtime: 5-15 minutes
#
# 3. Blue-Green Deployment (Minimal Risk):
#    - Deploy new cluster parallel to existing
#    - Test thoroughly before cutover
#    - Instant rollback capability
#    - Downtime: <1 minute (DNS/traffic switch)
#
# 4. Canary Upgrade (Gradual Rollout):
#    - Start with 1 pod, gradually expand
#    - Monitor each stage before proceeding
#    - Early issue detection
#    - Downtime: None (distributed mode)
#
# Pre-Upgrade Checklist:
#   make -f make/ops/minio.mk minio-pre-upgrade-check   # Validation
#   make -f make/ops/minio.mk minio-full-backup         # Backup all components
#
# Post-Upgrade Validation:
#   make -f make/ops/minio.mk minio-post-upgrade-check  # Automated validation
#
# Rollback:
#   make -f make/ops/minio.mk minio-upgrade-rollback    # Helm rollback
#
# Version-Specific Notes:
# - RELEASE.2023.x → 2024.x: Review mc client compatibility
# - RELEASE.2022.x → 2024.x: Test bucket versioning, lifecycle policies
# - Distributed Mode Scaling: Maintain even number of drives per set
# - Erasure Coding Changes: Verify data redundancy settings
# ==================================================================================
upgrade:
  # enabled: false - Documentation only, upgrades are manual via Helm
  # Always backup before upgrading: make -f make/ops/minio.mk minio-full-backup
  preUpgradeBackup: true  # Strongly recommended
  documentation:
    strategies:
      rolling:
        description: "Zero downtime upgrade for distributed mode (4+ nodes)"
        downtime: "None"
        steps:
          - "Verify distributed mode (replicaCount >= 4)"
          - "Set updateStrategy.type=RollingUpdate"
          - "helm upgrade with new image.tag"
          - "Monitor rollout status"
          - "Validate cluster health"
      in_place:
        description: "Planned downtime upgrade for standalone mode (1 node)"
        downtime: "5-15 minutes"
        steps:
          - "Schedule maintenance window"
          - "helm upgrade with new image.tag"
          - "Wait for pod restart"
          - "Validate service health"
      blue_green:
        description: "Parallel cluster deployment with traffic switch"
        downtime: "<1 minute (traffic switch)"
        steps:
          - "Deploy new cluster (different namespace)"
          - "Sync data (site replication or mc mirror)"
          - "Test new cluster thoroughly"
          - "Switch traffic (update DNS/ingress)"
          - "Monitor new cluster"
          - "Decommission old cluster after validation"
      canary:
        description: "Gradual rollout starting with 1 pod"
        downtime: "None (distributed mode)"
        steps:
          - "Set updateStrategy.rollingUpdate.partition to control rollout"
          - "Upgrade 1 pod, monitor health"
          - "Gradually decrease partition value"
          - "Monitor cluster health at each stage"
          - "Complete rollout when validated"
