# Prometheus - Production-Ready Example Configuration
# Copy and customize for your environment

# =============================================================================
# Deployment Configuration
# =============================================================================
# 2 replicas for HA (both scrape all targets independently)
replicaCount: 2

image:
  repository: prom/prometheus
  pullPolicy: IfNotPresent
  tag: ""  # Defaults to chart appVersion (v2.48.1)

# =============================================================================
# Prometheus Configuration
# =============================================================================
prometheus:
  # Global scrape settings
  global:
    scrapeInterval: "15s"  # How often to scrape targets
    evaluationInterval: "15s"  # How often to evaluate rules
    externalLabels:
      cluster: "production"
      environment: "prod"
      region: "us-east-1"

  # Data retention
  retention:
    time: "30d"  # Keep data for 30 days
    size: "90GB"  # Max TSDB size (leave headroom from storage)

  # Admin API (enable for snapshot/TSDB operations)
  enableAdminAPI: false  # Set true if needed for operations

  # Kubernetes service discovery
  kubernetesSD:
    enabled: true  # Auto-discover pods, services, endpoints

  # Alertmanager integration
  alerting:
    enabled: true
    alertmanagers:
      - alertmanager.monitoring.svc.cluster.local:9093
      # - alertmanager-1.monitoring.svc.cluster.local:9093  # HA setup

  # Alert and recording rule files
  ruleFiles:
    - /etc/prometheus/rules/*.yml

  # Additional scrape configurations
  additionalScrapeConfigs: []
    # - job_name: 'custom-app'
    #   kubernetes_sd_configs:
    #     - role: pod
    #   relabel_configs:
    #     - source_labels: [__meta_kubernetes_pod_label_app]
    #       action: keep
    #       regex: my-app

  # Custom prometheus.yml sections
  config: {}
    # remote_write:
    #   - url: "https://metrics-remote.example.com/api/v1/write"
    #     queue_config:
    #       capacity: 10000
    #       max_shards: 50

  # Extra CLI arguments
  extraArgs:
    - --web.external-url=http://prometheus.example.com
    - --storage.tsdb.min-block-duration=2h
    - --storage.tsdb.max-block-duration=2h

  # Mount alert/recording rules from ConfigMap
  extraVolumes:
    - name: rules
      configMap:
        name: prometheus-rules
        optional: true

  extraVolumeMounts:
    - name: rules
      mountPath: /etc/prometheus/rules

# =============================================================================
# RBAC Configuration
# =============================================================================
# Required for Kubernetes service discovery
rbac:
  create: true
  # ClusterRole permissions for service discovery
  clusterRoleRules:
    - apiGroups: [""]
      resources:
        - nodes
        - nodes/proxy
        - nodes/metrics
        - services
        - endpoints
        - pods
      verbs: ["get", "list", "watch"]
    - apiGroups: [""]
      resources:
        - configmaps
      verbs: ["get"]
    - apiGroups: ["networking.k8s.io"]
      resources:
        - ingresses
      verbs: ["get", "list", "watch"]

# =============================================================================
# Security Configuration
# =============================================================================
serviceAccount:
  create: true
  annotations: {}
  name: ""

podSecurityContext:
  fsGroup: 65534
  runAsUser: 65534
  runAsNonRoot: true

securityContext:
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 65534

# =============================================================================
# Service Configuration
# =============================================================================
service:
  type: ClusterIP
  port: 9090
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"

# =============================================================================
# Resource Configuration
# =============================================================================
# Production resources (adjust based on cardinality)
resources:
  limits:
    cpu: 4000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

# =============================================================================
# Health Probes
# =============================================================================
livenessProbe:
  httpGet:
    path: /-/healthy
    port: http
  initialDelaySeconds: 45
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /-/ready
    port: http
  initialDelaySeconds: 45
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3

startupProbe:
  httpGet:
    path: /-/ready
    port: http
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 30

# =============================================================================
# Storage Configuration
# =============================================================================
persistence:
  enabled: true
  accessMode: ReadWriteOnce
  size: 100Gi  # Adjust based on retention and cardinality
  storageClass: ""  # Use "fast-ssd" for production

# =============================================================================
# High Availability Configuration
# =============================================================================
# Pod anti-affinity (spread Prometheus instances across nodes)
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - prometheus
          topologyKey: kubernetes.io/hostname

nodeSelector: {}
  # node-role.kubernetes.io/monitoring: "true"

tolerations: []

# =============================================================================
# Ingress Configuration
# =============================================================================
ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
    nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
  hosts:
    - host: prometheus.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: prometheus-tls
      hosts:
        - prometheus.example.com

# =============================================================================
# ServiceMonitor (Prometheus Operator CRD)
# =============================================================================
serviceMonitor:
  enabled: true
  namespace: ""  # Deploy in same namespace
  labels:
    release: prometheus
  interval: 30s
  scrapeTimeout: 10s

# =============================================================================
# Pod Configuration
# =============================================================================
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"

podLabels:
  environment: "production"

# =============================================================================
# Notes
# =============================================================================
# Production Recommendations:
# 1. Enable authentication for ingress (basic auth or OAuth)
# 2. Configure remote_write for long-term storage
# 3. Set up alert rules in ConfigMap (prometheus-rules)
# 4. Monitor Prometheus itself (self-scraping enabled by default)
# 5. Plan retention based on cardinality and query patterns
# 6. Use recording rules to pre-aggregate high-cardinality data
# 7. Enable admin API only when needed for operations
# 8. Configure proper RBAC for service discovery
# 9. Set appropriate resource limits based on metrics volume
# 10. Use fast SSD storage for TSDB performance
#
# High Availability Setup:
# - 2 replicas scrape all targets independently
# - Each replica has its own TSDB
# - Deduplication handled by query layer (Thanos/Cortex) or clients
# - Not true HA clustering, but provides redundancy
#
# Storage Calculation:
# - Ingestion rate: ~1000 samples/sec = ~2.5GB/day
# - 30-day retention = ~75GB (leave headroom)
# - Adjust based on actual cardinality
#
# Service Discovery:
# - kubernetes_sd_configs: Auto-discover K8s resources
# - Relabeling: Filter and transform discovered targets
# - ServiceMonitor CRD: Declarative monitoring (Prometheus Operator)
#
# Alert Rules Example:
# Create ConfigMap prometheus-rules:
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: prometheus-rules
#   data:
#     alerts.yml: |
#       groups:
#         - name: node
#           rules:
#             - alert: HighMemoryUsage
#               expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < 0.1
#               for: 5m
#               labels:
#                 severity: warning
#               annotations:
#                 summary: "High memory usage detected"
#
# Remote Write Example (long-term storage):
#   config:
#     remote_write:
#       - url: "https://prometheus-remote-storage.example.com/api/v1/write"
#         basic_auth:
#           username: "prometheus"
#           password_file: /etc/secrets/remote-password
#
# Performance Tuning:
# - Increase scrape interval for high-cardinality targets
# - Use recording rules to pre-compute expensive queries
# - Limit label cardinality (avoid unbounded labels like user IDs)
# - Configure appropriate retention time and size
