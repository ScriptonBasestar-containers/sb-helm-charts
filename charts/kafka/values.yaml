# Default values for kafka.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Number of Kafka brokers
replicaCount: 1

image:
  repository: bitnami/kafka
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Kafka configuration
kafka:
  # KRaft cluster ID (auto-generated if empty)
  clusterId: ""

  # Process roles: broker, controller, or broker,controller (combined mode)
  processRoles: "broker,controller"

  # Controller quorum voters (for KRaft mode)
  # Format: id1@host1:port1,id2@host2:port2,...
  # Will be auto-generated based on replicaCount if empty
  controllerQuorumVoters: "0@kafka-0.kafka-headless.default.svc.cluster.local:9093"

  # Listeners configuration
  listeners: "PLAINTEXT://:9092,CONTROLLER://:9093"
  advertisedListeners: "PLAINTEXT://kafka-0.kafka-headless.default.svc.cluster.local:9092"
  listenerSecurityProtocolMap: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
  controllerListenerNames: "CONTROLLER"
  interBrokerListenerName: "PLAINTEXT"

  # Log directories
  logDirs: "/bitnami/kafka/data"

  # Topic defaults
  numPartitions: 1
  defaultReplicationFactor: 1
  minInsyncReplicas: 1
  autoCreateTopicsEnable: true

  # Log retention
  logRetentionHours: 168  # 7 days
  logRetentionBytes: -1  # unlimited
  logSegmentBytes: 1073741824  # 1GB
  logCleanupPolicy: "delete"

  # Performance tuning
  numNetworkThreads: 3
  numIoThreads: 8
  socketSendBufferBytes: 102400
  socketReceiveBufferBytes: 102400
  socketRequestMaxBytes: 104857600  # 100MB

  # Replication
  replicaFetchMaxBytes: 1048576  # 1MB
  replicaSocketTimeoutMs: 30000

  # SASL authentication
  sasl:
    enabled: false
    mechanisms: "PLAIN"
    interBrokerProtocol: "PLAIN"
    username: "admin"
    password: ""  # Set this for production

  # Additional kafka configuration
  # Example:
  # config:
  #   compression.type: "gzip"
  #   message.max.bytes: "1000000"
  config: {}

  # Use existing ConfigMap for Kafka configuration
  existingConfigMap: ""

  # Use existing Secret for credentials
  existingSecret: ""

  # Extra environment variables
  extraEnv: []
  #  - name: KAFKA_HEAP_OPTS
  #    value: "-Xmx1G -Xms1G"

  # Extra volumes
  extraVolumes: []

  # Extra volume mounts
  extraVolumeMounts: []

# Kafka UI configuration
kafkaUI:
  enabled: true
  replicaCount: 1

  image:
    repository: provectuslabs/kafka-ui
    tag: "latest"
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 8080

  ingress:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: kafka-ui.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
    #  - secretName: kafka-ui-tls
    #    hosts:
    #      - kafka-ui.local

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi

  extraEnv: []

# Service configuration
service:
  type: ClusterIP
  port: 9092
  nodePort: ""
  loadBalancerIP: ""
  clusterIP: ""
  annotations: {}
  labels: {}

# Persistence configuration
persistence:
  enabled: true
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 10Gi
  annotations: {}

# ServiceAccount configuration
serviceAccount:
  create: true
  annotations: {}
  name: ""

# Pod annotations
podAnnotations: {}

# Pod security context
podSecurityContext:
  fsGroup: 1001
  runAsUser: 1001
  runAsNonRoot: true

# Container security context
securityContext:
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false

# Resource limits and requests
resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 250m
    memory: 512Mi

# Liveness probe configuration
livenessProbe:
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6

# Readiness probe configuration
readinessProbe:
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Startup probe configuration
startupProbe:
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 30

# Node selector
nodeSelector: {}

# Tolerations
tolerations: []

# Affinity
affinity: {}

# =============================================================================
# RBAC Configuration
# =============================================================================
rbac:
  # Create RBAC resources (Role, RoleBinding)
  create: true

  # Annotations for RBAC resources
  annotations: {}

# =============================================================================
# Backup & Recovery Configuration
# =============================================================================
# Note: Backup is handled via Makefile targets, not automated CronJobs
# This section documents the backup strategy and available tools
backup:
  # Enable backup documentation (does not create automated backups)
  enabled: false

  # Backup strategy documentation
  documentation:
    # Three-component backup strategy
    strategy: "kafka_topics_metadata + kafka_configs + data_volumes"

    # Tools used for backup
    tools:
      - "kafka-topics --describe"
      - "kafka-configs --describe"
      - "volumesnapshot (PVC)"
      - "kafka-reassign-partitions"

    # Backup components
    components:
      # 1. Topic metadata (topic names, partitions, replication factor, configs)
      topics_metadata: "kafka-topics --describe --bootstrap-server"

      # 2. Kafka broker configurations
      broker_configs: "kafka-configs --describe --entity-type brokers"

      # 3. Data volumes (log.dirs - actual message data)
      data_volumes: "PVC VolumeSnapshot or disk-level backup"

      # 4. Consumer group offsets (optional)
      consumer_offsets: "kafka-consumer-groups --describe"

      # 5. ACLs (if SASL/authorization enabled)
      acls: "kafka-acls --list --bootstrap-server"

    # RTO/RPO targets
    targets:
      rto: "< 60 minutes (metadata), < 4 hours (full recovery)"
      rpo: "24 hours (daily backups)"

    # Retention policy
    retention:
      daily: "30 days"
      weekly: "90 days"
      monthly: "1 year"

# =============================================================================
# Upgrade Configuration
# =============================================================================
# Note: Upgrade is a manual process with health checks and rollback support
# This section documents the upgrade workflow
upgrade:
  # Enable upgrade documentation (does not trigger automated upgrades)
  enabled: false

  # Pre-upgrade backup (recommended)
  preUpgradeBackup: true

  # Upgrade workflow documentation
  documentation:
    # Pre-upgrade checklist
    preUpgrade:
      - "Run kafka-pre-upgrade-check (broker health, ISR status)"
      - "Backup topic metadata and configurations"
      - "Create PVC snapshots for data volumes"
      - "Review Kafka release notes for breaking changes"
      - "Test upgrade in staging environment"

    # Upgrade strategies
    strategies:
      # 1. Rolling upgrade (recommended for minor versions)
      rolling:
        description: "Update brokers one by one with controlled-shutdown"
        downtime: "None (zero-downtime)"
        complexity: "Medium"
        steps:
          - "Update StatefulSet with new image"
          - "Kafka controller will perform rolling restart"
          - "Verify ISR status after each broker restart"

      # 2. Blue-green upgrade (for major versions)
      blue_green:
        description: "Deploy new cluster, migrate topics, switch traffic"
        downtime: "Minimal (brief cutover)"
        complexity: "High"
        steps:
          - "Deploy new Kafka cluster (green)"
          - "Set up MirrorMaker 2.0 for topic replication"
          - "Validate data consistency"
          - "Update producer/consumer configs to new cluster"
          - "Decommission old cluster after validation"

      # 3. Maintenance window upgrade
      maintenance:
        description: "Stop all brokers, upgrade, restart"
        downtime: "15-30 minutes"
        complexity: "Low"
        steps:
          - "Scale StatefulSet to 0 replicas"
          - "Update StatefulSet with new image and configs"
          - "Scale StatefulSet back to desired replicas"
          - "Verify cluster health and ISR status"

    # Post-upgrade validation
    postUpgrade:
      - "Run kafka-post-upgrade-check (cluster health)"
      - "Verify all brokers in-sync (ISR == replication factor)"
      - "Test topic produce/consume"
      - "Check Kafka UI connectivity"
      - "Monitor under-replicated partitions"

    # Rollback procedures
    rollback:
      helm: "helm rollback kafka (chart only, not data)"
      pvc_restore: "Restore PVC from VolumeSnapshot"
      full_recovery: "Uninstall + reinstall with PVC restore"
