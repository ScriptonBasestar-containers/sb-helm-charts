# Uptime Kuma values for Production - Single Instance (HA Not Supported)
# Production-grade configuration for enterprise monitoring
# Use: helm install uptime-kuma ./charts/uptime-kuma -f charts/uptime-kuma/values-prod-master-replica.yaml
#
# ⚠️ IMPORTANT: Uptime Kuma does NOT support horizontal scaling (HA)
#    SQLite limitations prevent multi-replica deployments
#    For high availability, use external MariaDB (experimental in v2.0+)
#
# Features:
# - SQLite database (production-grade with backups)
# - Single replica with PodDisruptionBudget
# - Production resources for 500+ monitors
# - Ingress with TLS enabled
# - Monitoring interval: 15s (fast response)
# - Network policies enabled
# - SMTP notifications configured

# ========================================
# Uptime Kuma Configuration
# ========================================
uptimeKuma:
  # Port configuration
  port: 3001
  host: "::"  # IPv4+IPv6 support

  # Data directory (inside container)
  dataDir: "/app/data"

  # Database configuration
  # Note: For production HA, consider MariaDB (experimental in v2.0+)
  database:
    type: "sqlite"  # SQLite for production (with backup strategy)

    # MariaDB configuration (EXPERIMENTAL - for future HA setup)
    # mariadb:
    #   host: "mariadb.database.svc.cluster.local"
    #   port: 3306
    #   database: "uptime_kuma"
    #   username: "uptime_kuma"
    #   password: ""  # Set via --set or existingSecret

  # SSL/TLS - disabled (handled by ingress)
  ssl:
    enabled: false

  # Security settings
  security:
    wsOriginCheck: "cors-like"
    disableFrameSameOrigin: false

  # Logging - enabled for production debugging
  logging:
    hideLog: ""
    sqlLog: false
    responseBodyMonitorId: ""

# ========================================
# Storage Configuration
# ========================================

# Persistence - production-grade storage
persistence:
  enabled: true
  storageClass: ""  # Use fast-ssd or premium storage class
  accessMode: ReadWriteOnce
  size: 10Gi  # Large capacity for extensive history
  mountPath: /app/data
  annotations:
    # Add backup annotations if using Velero
    backup.velero.io/backup-volumes: "data"

# ========================================
# Deployment Configuration
# ========================================

# Single instance (Uptime Kuma doesn't support HA)
# Note: PodDisruptionBudget ensures controlled disruption
replicaCount: 1

strategy:
  type: Recreate  # Required for RWO volume

# Container image
image:
  repository: louislam/uptime-kuma
  pullPolicy: IfNotPresent
  tag: ""  # Defaults to Chart.appVersion

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Service account
serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

podAnnotations:
  prometheus.io/scrape: "false"  # Uptime Kuma doesn't expose Prometheus metrics natively

podLabels:
  environment: "production"
  tier: "monitoring"
  criticality: "high"

# Security context
podSecurityContext:
  fsGroup: 1000
  runAsNonRoot: true
  runAsUser: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false  # Uptime Kuma needs write access
  runAsNonRoot: true
  runAsUser: 1000

# ========================================
# Service & Networking
# ========================================

# Service
service:
  type: ClusterIP
  port: 3001
  targetPort: 3001
  annotations: {}

# Ingress - ENABLED with TLS for production
ingress:
  enabled: true
  className: "nginx"
  annotations:
    # WebSocket support (required for Uptime Kuma)
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    nginx.ingress.kubernetes.io/websocket-services: "uptime-kuma"
    # TLS configuration
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    # Security headers
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "X-Frame-Options: SAMEORIGIN";
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "X-XSS-Protection: 1; mode=block";
      more_set_headers "Referrer-Policy: strict-origin-when-cross-origin";
    # Rate limiting
    nginx.ingress.kubernetes.io/limit-rps: "20"
    nginx.ingress.kubernetes.io/limit-connections: "10"
  hosts:
    - host: status.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: uptime-kuma-tls
      hosts:
        - status.example.com

# ========================================
# Resource Limits
# ========================================

# Production-grade resources
resources:
  limits:
    cpu: 1000m     # 1 CPU core
    memory: 1Gi    # 1GB RAM
  requests:
    cpu: 200m      # Guaranteed 200m
    memory: 512Mi  # Guaranteed 512Mi

# ========================================
# Health Checks
# ========================================

livenessProbe:
  enabled: true
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 6

readinessProbe:
  enabled: true
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3

startupProbe:
  enabled: true
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 0
  periodSeconds: 5
  timeoutSeconds: 5
  failureThreshold: 60  # Increased for production startup
  successThreshold: 1

# ========================================
# High Availability (Limited Support)
# ========================================

# Autoscaling - DISABLED (Uptime Kuma doesn't support horizontal scaling)
autoscaling:
  enabled: false

# Pod Disruption Budget - ENABLED for controlled disruption
# Even with single replica, this prevents accidental deletion during node drains
podDisruptionBudget:
  enabled: true
  minAvailable: 1

# ========================================
# Security
# ========================================

# Network Policy - ENABLED for production security
networkPolicy:
  enabled: true
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow from ingress controller
    - from:
      - namespaceSelector:
          matchLabels:
            name: ingress-nginx
      ports:
        - protocol: TCP
          port: 3001
    # Allow from monitoring namespace (if applicable)
    - from:
      - namespaceSelector:
          matchLabels:
            name: monitoring
      ports:
        - protocol: TCP
          port: 3001
  egress:
    # Allow DNS resolution
    - to:
      - namespaceSelector: {}
      ports:
        - protocol: UDP
          port: 53
    # Allow HTTPS for monitoring external services
    - to:
      - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 443
    # Allow HTTP for monitoring
    - to:
      - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 80
    # Allow SMTP for notifications
    - to:
      - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 587
        - protocol: TCP
          port: 465
        - protocol: TCP
          port: 25

# ========================================
# Monitoring
# ========================================

# Monitoring - OPTIONAL (Uptime Kuma doesn't expose Prometheus metrics natively)
# Use external monitoring to monitor Uptime Kuma itself
monitoring:
  enabled: false
  serviceMonitor:
    enabled: false
    interval: 30s
    path: /metrics
    labels:
      prometheus: kube-prometheus

# ========================================
# Node Selection
# ========================================

# Node selector for production workloads
nodeSelector: {}
# Example:
# nodeSelector:
#   node.kubernetes.io/instance-type: "n1-standard-2"
#   workload: "monitoring"

tolerations: []

# Affinity - not applicable for single instance
affinity: {}

topologySpreadConstraints: []

priorityClassName: "system-cluster-critical"  # High priority for monitoring

# ========================================
# Additional Configuration
# ========================================

# Extra environment variables for production
extraEnv:
  - name: TZ
    value: "UTC"
  # Node.js production optimizations
  - name: NODE_ENV
    value: "production"
  # Increase Node.js memory limit for large deployments
  - name: NODE_OPTIONS
    value: "--max-old-space-size=1024"

extraEnvFrom: []

# Additional volumes for SSL certificates (if using internal SSL)
extraVolumes: []
# Example:
# extraVolumes:
#   - name: ssl-certs
#     secret:
#       secretName: uptime-kuma-internal-ssl

# Additional volume mounts
extraVolumeMounts: []
# Example:
# extraVolumeMounts:
#   - name: ssl-certs
#     mountPath: /etc/ssl/certs
#     readOnly: true

# Init containers
initContainers: []

# Lifecycle hooks
lifecycle:
  preStop:
    exec:
      command: ["/bin/sh", "-c", "sleep 15"]  # Graceful shutdown delay

# ========================================
# Production Deployment Notes
# ========================================

# Pre-deployment checklist:
#
# 1. Infrastructure requirements:
#    ✓ Fast SSD storage class configured
#    ✓ DNS records configured
#    ✓ TLS certificates ready (Let's Encrypt)
#    ✓ Load balancer configured
#
# 2. Security requirements:
#    ✓ NetworkPolicy configured
#    ✓ Secrets stored in Kubernetes secrets
#    ✓ Regular security updates planned
#
# 3. Backup strategy:
#    ✓ Automated SQLite backups configured
#    ✓ Offsite backup storage (S3, GCS)
#    ✓ Backup retention policy defined
#    ✓ Restore procedure tested
#
# 4. Monitoring requirements:
#    ✓ External monitoring of Uptime Kuma itself
#    ✓ Alerting configured for downtime
#    ✓ Log aggregation enabled
#
# Post-deployment tasks:
#
# 1. Initial configuration:
#    - Create admin account with strong password
#    - Enable 2FA for admin account
#    - Configure SMTP for notifications
#
# 2. Set up monitoring targets:
#    - Critical production services (interval: 15s)
#    - Internal APIs and databases
#    - External dependencies
#    - SSL certificate expiration monitoring
#
# 3. Configure notification channels:
#    - Email (SMTP): admin team
#    - Slack/Teams: operations channel
#    - PagerDuty/OpsGenie: on-call rotation
#    - Webhook: incident management system
#    - SMS: critical alerts only
#
# 4. Create status pages:
#    - Public status page for customers
#    - Internal status page for team
#    - Custom domain and branding
#    - Incident history enabled
#
# 5. Set up backup automation:
#    - Daily SQLite backups
#    - Weekly full backups
#    - Monthly archive to cold storage
#    - Test restore procedure quarterly
#
# Backup automation example:
#
# 1. Create backup CronJob:
#    apiVersion: batch/v1
#    kind: CronJob
#    metadata:
#      name: uptime-kuma-backup
#    spec:
#      schedule: "0 2 * * *"  # Daily at 2 AM
#      jobTemplate:
#        spec:
#          template:
#            spec:
#              containers:
#              - name: backup
#                image: amazon/aws-cli
#                command:
#                - /bin/sh
#                - -c
#                - |
#                  kubectl exec uptime-kuma-xxx -- cp /app/data/kuma.db /app/data/kuma.db.backup
#                  kubectl cp uptime-kuma-xxx:/app/data/kuma.db.backup /tmp/kuma-$(date +%Y%m%d).db
#                  aws s3 cp /tmp/kuma-$(date +%Y%m%d).db s3://backups/uptime-kuma/
#              restartPolicy: OnFailure
#
# 2. Manual backup command:
#    kubectl exec -it uptime-kuma-xxx -- sqlite3 /app/data/kuma.db ".backup /app/data/kuma-backup.db"
#    kubectl cp uptime-kuma-xxx:/app/data/kuma-backup.db ./kuma-backup-$(date +%Y%m%d).db
#
# 3. Restore procedure:
#    kubectl cp ./kuma-backup.db uptime-kuma-xxx:/app/data/kuma.db
#    kubectl rollout restart deployment/uptime-kuma
#
# High Availability considerations:
#
# ⚠️ LIMITATION: Uptime Kuma does NOT support true HA with SQLite
#
# Options for improving availability:
#
# 1. External MariaDB (experimental in v2.0+):
#    - Requires migration from SQLite
#    - Supports multi-replica deployment
#    - Complex setup and maintenance
#
# 2. Automated failover:
#    - Use Velero for PVC snapshots
#    - Automate restore on failure
#    - RTO: ~5-10 minutes
#
# 3. Standby instance:
#    - Deploy secondary instance (different cluster)
#    - Sync SQLite database regularly
#    - Manual switchover on failure
#    - RTO: ~15-30 minutes
#
# 4. PodDisruptionBudget (current setup):
#    - Prevents accidental deletion
#    - Controls node drain behavior
#    - Does NOT provide HA
#    - RTO: depends on node recovery
#
# Scaling limits:
#
# - Tested with 1000+ monitors
# - CPU usage: ~50m per 100 monitors (at 60s interval)
# - Memory usage: ~100MB base + ~50MB per 100 monitors
# - SQLite database: ~1MB per 1000 status records
# - Status page views: ~10MB RAM per concurrent user
#
# Performance tuning:
#
# - Use fast SSD for storage (NVMe preferred)
# - Monitor CPU usage and adjust resources
# - Archive old data periodically (manual SQL)
# - Use CDN for public status pages
# - Consider monitoring interval vs load tradeoff
#
# Security best practices:
#
# 1. Enable 2FA for all accounts
# 2. Use strong passwords (min 16 characters)
# 3. Restrict status page access with password
# 4. Enable rate limiting on ingress
# 5. Regular security updates
# 6. Audit notification webhooks
# 7. Monitor login attempts
# 8. Use HTTPS only (no HTTP redirect exceptions)
